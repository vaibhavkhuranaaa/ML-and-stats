{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi_wIVzzbJIU"
      },
      "source": [
        "# **IDS575: Machine Learning and Statistical Methods**\n",
        "## [Quiz #03 - Logistic Regression and Binary Classification]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs-KJZ7jbJIh"
      },
      "source": [
        "## Import Libraries\n",
        "* See various conventions and acronyms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAKMrSQubJIh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dINsxbBvbJIi"
      },
      "source": [
        "## Load the data into a DataFrame\n",
        "* Read directly from a csv (excel-like) data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxTBFShOc9xp",
        "outputId": "516ac9ab-c918-4995-e881-a893db32a7c7"
      },
      "outputs": [],
      "source": [
        "FraudDataset = pd.read_csv('C:/Users/Vaibhav Khurana/Desktop/MSBA/FALL 23/Assignments/IDS575/PA3/fraud.csv')\n",
        "print(type(FraudDataset))\n",
        "print(FraudDataset.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ooKj4Zwgtld"
      },
      "source": [
        "## Verify basic data statistics\n",
        "* Count the number of features. (i.e., attributes)\n",
        "* Count the number of examples. (i.e., instances and labels)\n",
        "* Unfortunately we don't know what each feature means due to privacy concerns.\n",
        "* Class variable: 0 (standard) / 1 (fradulent)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ycsrrcXeYx3",
        "outputId": "f49749c1-9d68-44c3-8b8b-af3eb592a05a"
      },
      "outputs": [],
      "source": [
        "def printBasicStats(dataset):\n",
        "  print('- # of features = %d' % (len(dataset.keys()) - 1))\n",
        "  print('- # of examples = %d' % len(dataset))\n",
        "  \n",
        "printBasicStats(FraudDataset)\n",
        "print(FraudDataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-a_Fm2du-mT"
      },
      "source": [
        "## Data inspection\n",
        "* See the label imbalance.\n",
        "* Measure the baseline accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptx29gCt6mQo",
        "outputId": "ab0cc268-a9b4-450c-8fa8-c82593800673"
      },
      "outputs": [],
      "source": [
        "Counts = FraudDataset['Class'].value_counts()\n",
        "print(Counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaQuKmCXbJIj",
        "outputId": "81034c42-e3ee-4a32-8b82-b695580480f3"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', 10)\n",
        "print(FraudDataset.describe(exclude=None))\n",
        "\n",
        "Counts = FraudDataset['Class'].value_counts()\n",
        "print(Counts)\n",
        "\n",
        "BaseLineAcc = Counts[0]/(Counts[0] + Counts[1])\n",
        "print(BaseLineAcc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9noHAgdBbJIk"
      },
      "source": [
        "## Data inspection Part II.\n",
        "* Measure the correlation.\n",
        "* Let's draw heatmap as an intuitive visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GIfrBGP-bJIk",
        "outputId": "322f5899-f33f-46da-f59e-34780282c1e9"
      },
      "outputs": [],
      "source": [
        "print(FraudDataset.corr())\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(FraudDataset.corr(), cmap=sns.diverging_palette(220, 10, as_cmap=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY576FhOg7xa"
      },
      "source": [
        "## Data split\n",
        "* Must split into train and test data but with respect to the class distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljg5JPskg7Ma"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def splitTrainTest(df, size):\n",
        "  split = StratifiedShuffleSplit(n_splits=1, test_size=size, random_state=0)\n",
        "\n",
        "  # For each pair of train and test indices,\n",
        "  X = df.drop('Class', axis=1)\n",
        "  y = df.Class  \n",
        "  for trainIndexes, testIndexes in split.split(X, y):\n",
        "    X_train, y_train = X.iloc[trainIndexes], y.iloc[trainIndexes]\n",
        "    X_test, y_test = X.iloc[testIndexes], y.iloc[testIndexes]\n",
        "\n",
        "  return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = splitTrainTest(FraudDataset, 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TecDfoLCqFDi",
        "outputId": "24861d94-1d9d-4c7a-c715-9764052e30ad"
      },
      "outputs": [],
      "source": [
        "print(y_train.value_counts())\n",
        "print(y_test.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMcdoEH9-6Np"
      },
      "source": [
        "## Logistic Regression \n",
        "* Train the logistic regression.\n",
        "* Train again with normalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6At9tfyu_wcC",
        "outputId": "e3c61e38-ba68-4aaf-c787-ca449da7669a"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix \n",
        "\n",
        "def doLogisticRegression(X, y, normalize=False):\n",
        "  # If normalize option is enabled,\n",
        "  if normalize:\n",
        "    # For each feature (indexed by j as usual)\n",
        "    for j in X.columns:\n",
        "      # Subtract its column mean and update the value.\n",
        "      X[j] -= X[j].mean()\n",
        "\n",
        "      # Divide by its standard deviation and update the value.\n",
        "      X[j] /= X[j].std()\n",
        "\n",
        "  # Instanciate an object from Logistic Regression class.\n",
        "  lr = LogisticRegression()\n",
        "\n",
        "  # Perform training and prediction.\n",
        "  lr.fit(X, y)\n",
        "  y_pred = lr.predict(X)\n",
        "      \n",
        "  # Return training accuracy and confusion matrix.\n",
        "  return accuracy_score(y, y_pred), confusion_matrix(y, y_pred), lr\n",
        "\n",
        "TrainAcc, TrainConf, LR = doLogisticRegression(X_train, y_train, normalize=True)\n",
        "print(TrainAcc)\n",
        "print(TrainConf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = [[1],[2],[3],[4]]\n",
        "y = [1,1,0,0]\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Perform training and prediction.\n",
        "lr.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"a0: {lr.intercept_}\")\n",
        "print(f\"a1: {lr.coef_}\")\n",
        "print(f\"-a0/a1: {-lr.intercept_/lr.coef_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "-a0/a1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5LcgQphsA6n",
        "outputId": "e2965f36-5c06-46d3-f03b-31095b7c180c"
      },
      "outputs": [],
      "source": [
        "y_test_pred = LR.predict(X_test)\n",
        "TestAcc, TestConf = accuracy_score(y_test, y_test_pred), confusion_matrix(y_test, y_test_pred)\n",
        "print(TestAcc)\n",
        "print(TestConf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmzeCBoik7-b"
      },
      "source": [
        "# Programming Assignment (PA)\n",
        "*   Implement logistic()\n",
        "*   Implement logLikelihood()\n",
        "*   Implement predict()\n",
        "*   Implement miniBatchGradientDescent()\n",
        "*   Play with testYourCode() that compares your implementations agaisnt scikit-learn's results.\n",
        "*   Note that your log-likelihood must increase over epoch as you update the model parameter theta toward its maximum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKm17sOc91qE"
      },
      "outputs": [],
      "source": [
        "class MyLogisticRegression:\n",
        "  # Randomly initialize the parameter vector.\n",
        "  theta = None\n",
        "\n",
        "  def logistic(self, z):\n",
        "    # Return the sigmoid function value.\n",
        "    ##############################################################\n",
        "    # TO-DO: Complete the evaluation of logistic function given z.\n",
        "    logisticValue = 1 / (1 + np.exp(-z))\n",
        "    ##############################################################\n",
        "    return logisticValue\n",
        "\n",
        "  def logLikelihood(self, X, y):\n",
        "    # Compute the log-likelihood hood of all training examples.\n",
        "    # X: (m x (n+1)) data matrix\n",
        "    # y: (m x 1) output vector    \n",
        "\n",
        "    # If theta parameter has not trained yet,\n",
        "    if not isinstance(self.theta, np.ndarray):\n",
        "      return 0.0\n",
        "\n",
        "    # Compute the linear hypothesis given individual examples (as a whole).\n",
        "    h_theta = self.logistic(np.dot(X, self.theta))\n",
        "\n",
        "    # Evalaute the two terms in the log-likelihood.    \n",
        "    #################################################################\n",
        "    # TO-DO: Compute the two terms in the log-likelihood of the data.\n",
        "    probability1 = y * np.log(h_theta)\n",
        "    probability0 = (1 - y) * np.log(1 - h_theta)\n",
        "    #################################################################\n",
        "\n",
        "    # Return the average of the log-likelihood\n",
        "    m = X.shape[0]\n",
        "    return (1.0/m) * np.sum(probability1 + probability0) \n",
        "\n",
        "  def fit(self, X, y, alpha=0.01, epoch=50):\n",
        "    # Extract the data matrix and output vector as a numpy array from the data frame.\n",
        "    # Note that we append a column of 1 in the X for the intercept.\n",
        "    X = np.concatenate((np.array(X), np.ones((X.shape[0], 1), dtype=np.float64)), axis=1)\n",
        "    y = np.array(y)  \n",
        "\n",
        "    # Run mini-batch gradient descent.\n",
        "    self.miniBatchGradientDescent(X, y, alpha, epoch)\n",
        "\n",
        "  def predict(self, X):\n",
        "    # Extract the data matrix and output vector as a numpy array from the data frame.\n",
        "    # Note that we append a column of 1 in the X for the intercept.\n",
        "    X = np.concatenate((np.array(X), np.ones((X.shape[0], 1), dtype=np.float64)), axis=1)\n",
        "\n",
        "    # Perfrom a prediction only after a training happens.\n",
        "    if isinstance(self.theta, np.ndarray):\n",
        "      y_pred = self.logistic(X.dot(self.theta))\n",
        "      ####################################################################################\n",
        "      # TO-DO: Given the predicted probability value, decide your class prediction 1 or 0.\n",
        "      y_pred_class = (y_pred >= 0.5).astype(int)\n",
        "      ####################################################################################\n",
        "      return y_pred_class\n",
        "    return None\n",
        "\n",
        "  def miniBatchGradientDescent(self, X, y, alpha, epoch, batch_size=100):    \n",
        "    (m, n) = X.shape\n",
        "  \n",
        "    # Randomly initialize our parameter vector. (DO NOT CHANGE THIS PART!)\n",
        "    # Note that n here indicates (n+1) because X is already appended by the intercept term.\n",
        "    np.random.seed(2) \n",
        "    self.theta = 0.1*(np.random.rand(n) - 0.5)\n",
        "    print('L2-norm of the initial theta = %.4f' % np.linalg.norm(self.theta, 2))\n",
        "    \n",
        "    # Start iterations\n",
        "    for iter in range(epoch):\n",
        "      # Print out the progress report for every 1000 iteration.\n",
        "      if (iter % 5) == 0:\n",
        "        print('+ currently at %d epoch...' % iter)   \n",
        "        print('  - log-likelihood = %.4f' % self.logLikelihood(X, y))\n",
        "\n",
        "      # Create a list of shuffled indexes for iterating training examples.     \n",
        "      indexes = np.arange(m)\n",
        "      np.random.shuffle(indexes)\n",
        "\n",
        "      # For each mini-batch,\n",
        "      for i in range(0, m - batch_size + 1, batch_size):\n",
        "        # Extract the current batch of indexes and corresponding data and outputs.\n",
        "        indexSlice = indexes[i:i+batch_size]        \n",
        "        X_batch = X[indexSlice, :]\n",
        "        y_batch = y[indexSlice]\n",
        "\n",
        "        # For each feature\n",
        "        for j in np.arange(n):\n",
        "          gradient = np.dot(X_batch.T, (self.logistic(np.dot(X_batch, self.theta)) - y_batch)) / batch_size\n",
        "        self.theta -= alpha * gradient\n",
        "          ####################################################################################\n",
        "          # TO-DO: Perform like a batch gradient desceint within the current mini-batch.\n",
        "          # Note that your algorithm must update self.theta[j].\n",
        "                   \n",
        "          ####################################################################################\n",
        "          \n",
        "  \n",
        "def doMyLogisticRegression(X, y, alpha, epoch, normalize=False):\n",
        "  # If normalize option is enabled,\n",
        "  if normalize:\n",
        "    # For each feature (indexed by j as usual)\n",
        "    for j in X.columns:\n",
        "      # Subtract its column mean and update the value.\n",
        "      X[j] -= X[j].mean()\n",
        "\n",
        "      # Divide by its standard deviation and update the value.\n",
        "      X[j] /= X[j].std()\n",
        "\n",
        "  # Instanciate an object from Logistic Regression class.\n",
        "  lr = MyLogisticRegression()\n",
        "\n",
        "  # Perform training and prediction.\n",
        "  lr.fit(X, y, alpha, epoch,)\n",
        "  y_pred = lr.predict(X)\n",
        "      \n",
        "  # Return training accuracy and confusion matrix.\n",
        "  return accuracy_score(y, y_pred), confusion_matrix(y, y_pred), lr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PrBUMlklvVE",
        "outputId": "17797dab-a53b-456c-ff69-bbb8a77ae89b"
      },
      "outputs": [],
      "source": [
        "def testYourCode(X_train, y_train, X_test, y_test, alpha, epoch):\n",
        "  # Test the code with scikit-learn.\n",
        "  trainAcc, trainConf, lr = doLogisticRegression(X_train, y_train, normalize=True)\n",
        "  y_test_pred = lr.predict(X_test)\n",
        "  testAcc, testConf = accuracy_score(y_test, y_test_pred), confusion_matrix(y_test, y_test_pred)\n",
        "  print(\"Scikit's training/test accuracies = %.4f / %.4f\" % (trainAcc, testAcc))\n",
        "  print(\"Scikit's training/test confusion matrix\\n %s\\n %s\" % (trainConf, testConf))\n",
        "  theta = np.append(lr.coef_[0], lr.intercept_)\n",
        "  print(theta)\n",
        "\n",
        "  # Test the code with your own version.\n",
        "  myTrainAcc, myTrainConf, myLR = doMyLogisticRegression(X_train, y_train, alpha, epoch, normalize=True)\n",
        "  my_y_test_pred = myLR.predict(X_test)\n",
        "  myTestAcc, myTestConf = accuracy_score(y_test, my_y_test_pred), confusion_matrix(y_test, my_y_test_pred)\n",
        "  print(\"My training/test accuracies = %.4f / %.4f\" % (myTrainAcc, myTestAcc))\n",
        "  print(\"My training/test confusion matrix\\n %s\\n %s\" % (myTrainConf, myTestConf))\n",
        "  print(myLR.theta)\n",
        "\n",
        "testYourCode(X_train, y_train, X_test, y_test, 0.05, 100)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "IDS575 - Quiz #03 - Logistic Regression and Binary Classification (PA)",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
